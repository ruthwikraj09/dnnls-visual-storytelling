DNNS Final Assessment


#Introduction and Problem Statement 
The project repository includes the final project for the Deep Neural Networks & Learning Systems (DNNLS) course. The project revolves around the concept of enhancing a existing storytelling model using additional features and advancements.We've built a high-tech story prediction machine with all the latest AI features installed. 

#Problem Definition
The system integrates transformer-based cross-modal fusion with diffusion based generation, augmented by contrastive learning and curriculum training to achieve coherent multimodal story continuation.

Evaluation Metrics
For evaluating image quality, metrics like SSIM,FID, and CLIP score are used, and to check text generator performance,BLEU-4,ROUGE-L. Semantic understanding is measured on the tag prediction accuracy.

#Methods
Here is the overview of the methods we used :
We use semantic tags and an enhanced visual backbone in combination to effectively fuse image and text information over time with Cross-Modal Temporal Fusion (CMTT).
It ensures high-quality and consistent image generation via a latent diffusion decoder, supported by temporal contrastive loss and early stopping.
Performances are measured by SSIM, BLEU, and ROUGE, and the data is split into 80/10/10 for training, validation, and testing.

#Model Architecture Overview
The proposed architecture incorporates the use of enhanced visual encoder (CNN with batch normalization) and the BERT-LSTM text encoder with semantic tag detection, which are connected using the Cross-Modal Temporal Transformer (CMTT) for multimodal feature fusion. The latent diffusion decoder is utilized for high-quality image production, while multi-task learning is used for the joint optimization of image production, text representation, tag prediction, and temporal contrastive learning. The implementation of the architecture is enhanced using curriculum learning,contrastive loss,early stopping, while the evaluation of the model's performance employs SSIM, BLEU, ROUGE, and CLIP metrics.

# Reasoning aware attention
The proposed mechanism enables the model to selectively focus on relevant visual features , textual context and semantic tags at any point in time through logical and temporal relations .It has been implemented  in the Cross-Modal Temporal Transformer (CMTT), which helps in analyzing the effect and cause and maintains coherence in the generation of images and texts.


#Code Snippet(simplified)

# =========== ENHANCEMENT 1: CROSS-MODAL TEMPORAL TRANSFORMER ===========
class CrossModalTemporalTransformer(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_layers=4):
        super().__init__()
        self.d_model = d_model

        # Intra-modal temporal self-attention
        self.visual_self_attn = nn.MultiheadAttention(d_model, nhead)
        self.text_self_attn = nn.MultiheadAttention(d_model, nhead)

        # Cross-modal cross-attention
        self.visual_cross_attn = nn.MultiheadAttention(d_model, nhead)
        self.text_cross_attn = nn.MultiheadAttention(d_model, nhead)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, visual_seq, text_seq):
        # visual_seq: [seq_len, batch, d_model]
        # text_seq: [seq_len, batch, d_model]

        # Intra-modal self-attention
        visual_self, _ = self.visual_self_attn(visual_seq, visual_seq, visual_seq)
        text_self, _ = self.text_self_attn(text_seq, text_seq, text_seq)

        # Add & Norm
        visual_self = self.norm1(visual_seq + visual_self)
        text_self = self.norm1(text_seq + text_self)

        # Cross-modal attention
        visual_cross, _ = self.visual_cross_attn(visual_self, text_self, text_self)
        text_cross, _ = self.text_cross_attn(text_self, visual_self, visual_self)

        # Add & Norm
        visual_out = self.norm2(visual_self + visual_cross)
        text_out = self.norm2(text_self + text_cross)

        return visual_out, text_out

# Keep the original Attention class for backward compatibility
class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_dim, 1)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, rnn_outputs):
        energy = self.attn(rnn_outputs).squeeze(2)
        attn_weights = self.softmax(energy)
        context = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs)
        return context.squeeze(1)

# =========== ENHANCEMENT 4: ENHANCED SEQUENCEPREDICTOR WITH CMTT AND TAGS ===========
class EnhancedSequencePredictor(nn.Module):
    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim, gru_hidden_dim, tag_vocab_sizes=None):
        super().__init__()

        # Existing encoders
        self.image_encoder = visual_autoencoder.encoder
        self.text_encoder = text_autoencoder.encoder

        # Projections for CMTT
        self.visual_proj = nn.Linear(visual_autoencoder.encoder.projection[0].out_features, 512)
        self.text_proj = nn.Linear(text_autoencoder.encoder.hidden_dim, 512)

        # CMTT Enhancement
        self.cmtt = CrossModalTemporalTransformer(d_model=512, nhead=8, num_layers=2)

        # =========== TAG EMBEDDINGS ===========
        if tag_vocab_sizes is None:
            tag_vocab_sizes = {'objects': 1000, 'actions': 500, 'locations': 200}

        self.object_embedding = nn.Embedding(tag_vocab_sizes['objects'], 32, padding_idx=0)
        self.action_embedding = nn.Embedding(tag_vocab_sizes['actions'], 32, padding_idx=0)
        self.location_embedding = nn.Embedding(tag_vocab_sizes['locations'], 32, padding_idx=0)

        # Tag processing layers
        self.tag_processor = nn.Sequential(
            nn.Linear(32*3, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128)
        )

        # =========== ADD TAG PREDICTION HEAD ===========
        self.tag_pred_head = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, tag_vocab_sizes['objects'] + tag_vocab_sizes['actions'] + tag_vocab_sizes['locations'])
        )
        # =========== END TAG EMBEDDINGS ===========

        # Fusion layer after CMTT (now includes tag features)
        self.fusion = nn.Linear(512 * 2 + 128, latent_dim)

        # Keep existing components
        self.temporal_rnn = nn.GRU(latent_dim, gru_hidden_dim, batch_first=True)
        self.attention = Attention(gru_hidden_dim)
        self.projection = nn.Sequential(
            nn.Linear(gru_hidden_dim * 2, latent_dim),
            nn.ReLU()
        )

        # Decoders
        self.image_decoder = visual_autoencoder.decoder
        self.text_decoder = text_autoencoder.decoder

        # The text decoder's LSTM hidden_dim is 16
        self.fused_to_h0 = nn.Linear(latent_dim, text_autoencoder.decoder.hidden_dim)
        self.fused_to_c0 = nn.Linear(latent_dim, text_autoencoder.decoder.hidden_dim)

    def forward(self, image_seq, text_seq, objects_seq, actions_seq, locations_seq, target_seq):
        batch_size, seq_len, C, H, W = image_seq.shape

        # Encode sequences
        img_flat = image_seq.view(batch_size * seq_len, C, H, W)
        txt_flat = text_seq.view(batch_size * seq_len, -1)

        z_v_flat = self.image_encoder(img_flat)
        _, hidden_text_encoder, cell_text_encoder = self.text_encoder(txt_flat)

        # =========== PROCESS TAGS ===========
        # Reshape tags: (batch, seq_len, num_tags) -> (batch*seq_len, num_tags)
        objects_flat = objects_seq.view(batch_size * seq_len, -1)
        actions_flat = actions_seq.view(batch_size * seq_len, -1)
        locations_flat = locations_seq.view(batch_size * seq_len, -1)

        # Get embeddings for each tag (handle multiple tags per frame)
        # For objects: (batch*seq_len, 5) -> average embedding
        obj_emb = self.object_embedding(objects_flat)  # (batch*seq_len, 5, 32)
        obj_emb = obj_emb.mean(dim=1)  # Average over 5 objects

        # For actions: (batch*seq_len, 3) -> average embedding
        act_emb = self.action_embedding(actions_flat)  # (batch*seq_len, 3, 32)
        act_emb = act_emb.mean(dim=1)

        # For locations: (batch*seq_len, 2) -> average embedding
        loc_emb = self.location_embedding(locations_flat)  # (batch*seq_len, 2, 32)
        loc_emb = loc_emb.mean(dim=1)

        # Combine tag embeddings
        tag_emb = torch.cat([obj_emb, act_emb, loc_emb], dim=-1)  # (batch*seq_len, 96)
        tag_features = self.tag_processor(tag_emb)  # (batch*seq_len, 128)
        # =========== END TAG PROCESSING ===========

        # Project for CMTT
        z_v_proj = self.visual_proj(z_v_flat)
        z_t_proj = self.text_proj(hidden_text_encoder.squeeze(0))

        # Reshape for temporal processing
        z_v_seq = z_v_proj.view(batch_size, seq_len, -1).transpose(0, 1)
        z_t_seq = z_t_proj.view(batch_size, seq_len, -1).transpose(0, 1)
        tag_seq = tag_features.view(batch_size, seq_len, -1).transpose(0, 1)

        # Fuse modalities (including tags) - THIS IS THE CORRECT IMPLEMENTATION
        z_v_cmtt, z_t_cmtt = self.cmtt(z_v_seq, z_t_seq)
        z_v_fused = z_v_cmtt.mean(dim=0)
        z_t_fused = z_t_cmtt.mean(dim=0)
        tag_fused = tag_seq.mean(dim=0)

        z_fusion = torch.cat([z_v_fused, z_t_fused, tag_fused], dim=-1)
        z_final = self.fusion(z_fusion)

        # Continue with existing flow
        zseq, h = self.temporal_rnn(z_final.unsqueeze(1))
        h = h.squeeze(0)
        context = self.attention(zseq)
        z = self.projection(torch.cat((h, context), dim=1))

        # Decode
        pred_image_content = self.image_decoder(z)

        # Prepare initial hidden and cell states for text decoder
        h0 = self.fused_to_h0(z).unsqueeze(0)
        c0 = self.fused_to_c0(z).unsqueeze(0)

        decoder_input = target_seq[:, :-1]
        predicted_text_logits_k, _hidden, _cell = self.text_decoder(decoder_input, h0, c0)

        # =========== ADD TAG PREDICTIONS ===========
        tag_logits = self.tag_pred_head(tag_fused)
        # =========== END TAG PREDICTIONS ===========

        return pred_image_content, z, predicted_text_logits_k, z_final, tag_fused, tag_logits

#Results

The enhanced model produces better-quality images and more coherent predictions of the future compared to the baseline. CMTT enhances cross-modal temporal knowledge, diffusion decoding results in sharper and more detailed images, and semantic tags and contrastive loss promote more logical predictions of the future. The proposed improvements demonstrate improved story consistency, alignment, and early metric progress (SSIM, CLIP, and BLEU).

#Quantitavtive analysis


#Qualitative analysis



#conclusion
We would conclude that we've built a high-tech story prediction machine with all the latest AI features installed.

#Future work
Scaling up to more complex stories of variable lengths.
Multilingual story understanding and generation.
Video-based input instead of static images.







