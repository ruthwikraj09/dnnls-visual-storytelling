# -*- coding: utf-8 -*-
"""final_enhancing_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TDUVp-1V-RA9nQpYYEO6VtrdNbUgV39W

![banner.png](data:image/png;base64,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)

# Installing the Dependencies
"""

!pip install torchmetrics
# =========== ADD CLIP METRICS ===========
!pip install git+https://github.com/openai/CLIP.git

# @title Importing the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F

from bs4 import BeautifulSoup
import re
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
import os

from datasets import load_dataset

from datasets.fingerprint import random
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import torchvision.transforms.functional as FT

from transformers import BertTokenizer
import gc

import textwrap

# Add at the beginning (after other imports)
import math
# For evaluation metrics
try:
    from torchmetrics.text import BLEUScore, ROUGEScore
    from torchmetrics.image import StructuralSimilarityIndexMeasure
except:
    print("Warning: torchmetrics not installed")
    # Install it: !pip install torchmetrics

"""## ADD PERCEPTUAL LOSS"""

import torchvision.models as models

class PerceptualLoss(nn.Module):
    def __init__(self, device):
        super().__init__()
        # Use VGG16 features
        vgg = models.vgg16(pretrained=True).features[:16].eval()
        for param in vgg.parameters():
            param.requires_grad = False
        self.vgg = vgg.to(device)
        self.criterion = nn.L1Loss()

    def forward(self, pred, target):
        # Extract features
        pred_features = self.vgg(pred)
        target_features = self.vgg(target)
        return self.criterion(pred_features, target_features)

"""## ADDING CLIP EVALUATOR"""

import clip

class CLIPEvaluator:
    def __init__(self, device):
        self.device = device
        self.model, self.preprocess = clip.load("ViT-B/32", device=device)
        self.model.eval()

    def compute_score(self, images, texts):
        """Compute CLIP similarity between images and texts"""
        if images.dim() == 4:  # (batch, C, H, W)
            images = images
        else:
            images = images.unsqueeze(0)

        # Preprocess images if needed
        if images.max() > 1.0:
            images = images / 255.0

        with torch.no_grad():
            # Encode images and texts
            image_features = self.model.encode_image(images)
            text_tokens = clip.tokenize(texts).to(self.device)
            text_features = self.model.encode_text(text_tokens)

            # Normalize features
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)

            # Compute cosine similarity
            similarity = (image_features @ text_features.T).diag()

        return similarity.mean().item()

    def compute_image_retrieval(self, query_images, candidate_images, top_k=5):
        """Image-to-image retrieval"""
        with torch.no_grad():
            query_features = self.model.encode_image(query_images)
            candidate_features = self.model.encode_image(candidate_images)

            # Normalize
            query_features = query_features / query_features.norm(dim=-1, keepdim=True)
            candidate_features = candidate_features / candidate_features.norm(dim=-1, keepdim=True)

            # Compute similarity matrix
            similarity = query_features @ candidate_features.T

            # Get top-k matches
            top_k_scores, top_k_indices = similarity.topk(top_k, dim=1)

        return top_k_scores, top_k_indices

"""## HYPERPARAMETER SWEEP FUNCTION"""

# =========== ENHANCEMENT 9: Hyperparameter sweeps for γ, δ, model size, diffusion steps, batch size ===========
def hyperparameter_sweep():
    hyperparams = {
        'gamma': [0.1, 0.5, 1.0],  # Contrastive loss weight
        'delta': [0.01, 0.05, 0.1],  # Tag loss weight
        'model_size': ['small', 'medium', 'large'],
        'diffusion_steps': [50, 100, 200],
        'batch_size': [8, 16, 32]
    }

    for batch_size in hyperparams['batch_size']:
        for gamma in hyperparams['gamma']:
            print(f"Testing batch_size={batch_size}, gamma={gamma}")
            # Reinitialize dataloaders and model with new params
            # Train and evaluate
            # Record metrics

"""## ADD METRICS TRACKER"""

class TrainingMetrics:
    def __init__(self):
        self.metrics = {
            'train_loss': [],
            'val_loss': [],
            'train_ssim': [],
            'val_ssim': [],
            'train_bleu': [],
            'val_bleu': [],
            'train_rouge': [],
            'val_rouge': []
        }

    def update(self, phase, key, value):
        if f'{phase}_{key}' in self.metrics:
            self.metrics[f'{phase}_{key}'].append(value)

    def plot(self):
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))

        # Plot losses
        axes[0, 0].plot(self.metrics['train_loss'], label='Train Loss')
        axes[0, 0].plot(self.metrics['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # Plot SSIM
        axes[0, 1].plot(self.metrics['train_ssim'], label='Train SSIM')
        axes[0, 1].plot(self.metrics['val_ssim'], label='Val SSIM')
        axes[0, 1].set_title('SSIM (higher is better)')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # Plot BLEU
        axes[1, 0].plot(self.metrics['train_bleu'], label='Train BLEU')
        axes[1, 0].plot(self.metrics['val_bleu'], label='Val BLEU')
        axes[1, 0].set_title('BLEU Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # Plot ROUGE
        axes[1, 1].plot(self.metrics['train_rouge'], label='Train ROUGE')
        axes[1, 1].plot(self.metrics['val_rouge'], label='Val ROUGE')
        axes[1, 1].set_title('ROUGE Score')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.show()

# Create instance
metrics_tracker = TrainingMetrics()

"""## ADD EARLY STOPPING"""

class EarlyStopping:
    def __init__(self, patience=5, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return self.early_stop

early_stopper = EarlyStopping(patience=3)

"""# **The Data Preparation**


---

First we need to activate our google drive so that we can save out data permanently.

## 1.1 Loading and saving data
"""

# @title Setting up google drive to save checkpoints

# This will prompt you to authorize Google Drive access
drive.mount('/content/gdrive')

def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename="autoencoder_checkpoint.pth"):
    """
    Saves the checkpoint directly to a specified folder in your mounted Google Drive.
    """
    # 1. Define the full Google Drive path
    # 'DL_Checkpoints' is the folder you want to save to inside your Drive
    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'

    # Ensure the directory exists before attempting to save
    os.makedirs(drive_folder, exist_ok=True)

    # 2. Combine the folder and the filename
    full_path = os.path.join(drive_folder, filename)

    # 3. Create the checkpoint dictionary
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }

    # 4. Save the dictionary to the Google Drive path
    torch.save(checkpoint, full_path)
    print(f"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}")


def load_checkpoint_from_drive(model, optimizer=None, filename="autoencoder_checkpoint.pth"):
    """
    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).
    """
    # Define the same Google Drive folder path
    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'
    full_path = os.path.join(drive_folder, filename)

    # Check if the checkpoint file exists
    if not os.path.exists(full_path):
        raise FileNotFoundError(f"Checkpoint file not found: {full_path}")

    # Load the checkpoint
    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available

    # Restore model state
    model.load_state_dict(checkpoint['model_state_dict'])

    # Restore optimizer state (if provided)
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # Extract metadata
    epoch = checkpoint.get('epoch', 0)
    loss = checkpoint.get('loss', None)

    print(f"Checkpoint loaded from: {full_path} (epoch {epoch})")

    return model, optimizer, epoch, loss

# @title Functions to load images and process data


# This function just extracts the tags from the text, don't get distracted by it.
# I changed this function a bit to fix some bugs
def parse_gdi_text(text):
    """Parse GDI formatted text into structured data"""
    soup = BeautifulSoup(text, 'html.parser')
    images = []

    for gdi in soup.find_all('gdi'):
        # Debug: print what BeautifulSoup sees

        # Method 1: Try to get image attribute directly
        image_id = None
        if gdi.attrs:
            # Check for attributes like 'image1', 'image2', etc.
            for attr_name, attr_value in gdi.attrs.items():
                if 'image' in attr_name.lower():
                    image_id = attr_name.replace('image', '')
                    break

        # Method 2: Extract from the tag string using regex
        if not image_id:
            tag_str = str(gdi)
            match = re.search(r'<gdi\s+image(\d+)', tag_str)
            if match:
                image_id = match.group(1)

        # Method 3: Fallback - use sequential numbering
        if not image_id:
            image_id = str(len(images) + 1)

        content = gdi.get_text().strip()

        # Extract tagged elements using BeautifulSoup directly
        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]
        actions = [act.get_text().strip() for act in gdi.find_all('gda')]
        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]

        images.append({
            'image_id': image_id,
            'description': content,
            'objects': objects,
            'actions': actions,
            'locations': locations,
            'raw_text': str(gdi)
        })

    return images

# This is an utility function to show images.
# Why do we need to do all this?
def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):
  """
  De-normalize the image (if necessary) and show image
  """
  if de_normalize:
    new_mean = -img_mean/img_std
    new_std = 1/img_std

    image = transforms.Normalize(
        mean=new_mean,
        std=new_std
    )(image)
  ax.imshow(image.permute(1, 2, 0))

"""Now we load dataset from HuggingFace:"""

# @title Loading the dataset
from datasets import load_dataset
train_dataset = load_dataset("daniel3303/StoryReasoning", split="train")
test_dataset = load_dataset("daniel3303/StoryReasoning", split="test")

"""## 1.2 Three datasets


---



We will create three different dataset objects and the corresponding loaders for performing multiple tasks
"""

# @title Main dataset with semantic tags
class SequencePredictionDataset(Dataset):
    def __init__(self, original_dataset, tokenizer, curriculum_step=0):
        super(SequencePredictionDataset, self).__init__()
        self.dataset = original_dataset
        self.tokenizer = tokenizer
        self.curriculum_step = curriculum_step
        self.sequence_lengths = [2, 3, 4, 5]

        # Potential experiments: Try other transforms!
        self.transform = transforms.Compose([
            transforms.Resize((60, 125)),
            transforms.RandomHorizontalFlip(p=0.3),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

        # Initialize tag vocabulary
        self.tag_vocab = {'objects': {}, 'actions': {}, 'locations': {}}
        # Build vocabulary from entire dataset
        self._build_tag_vocabulary()

    def __len__(self):
        return len(self.dataset)

    def _build_tag_vocabulary(self):
        """Build vocabulary of all tags in the dataset"""
        print("Building tag vocabulary...")
        for idx in range(len(self.dataset)):
            story_text = self.dataset[idx]["story"]
            image_attributes = parse_gdi_text(story_text)

            for frame_attrs in image_attributes:
                # Add objects to vocabulary
                for obj in frame_attrs["objects"]:
                    if obj not in self.tag_vocab['objects']:
                        self.tag_vocab['objects'][obj] = len(self.tag_vocab['objects']) + 1

                # Add actions to vocabulary
                for act in frame_attrs["actions"]:
                    if act not in self.tag_vocab['actions']:
                        self.tag_vocab['actions'][act] = len(self.tag_vocab['actions']) + 1

                # Add locations to vocabulary
                for loc in frame_attrs["locations"]:
                    if loc not in self.tag_vocab['locations']:
                        self.tag_vocab['locations'][loc] = len(self.tag_vocab['locations']) + 1

        print(f"Vocabulary sizes - Objects: {len(self.tag_vocab['objects'])}, "
              f"Actions: {len(self.tag_vocab['actions'])}, "
              f"Locations: {len(self.tag_vocab['locations'])}")

    def _get_tag_idx(self, tag_type, tag):
        """Get index for a tag (returns 0 if tag not in vocabulary)"""
        return self.tag_vocab[tag_type].get(tag, 0)

    def __getitem__(self, idx):
        current_length = self.sequence_lengths[min(self.curriculum_step, len(self.sequence_lengths)-1)]

        # Adjust frame selection based on current_length
        if current_length < 5:
            # Use fewer frames
            frame_indices = np.random.choice(range(5), current_length, replace=False)
            frame_indices.sort()
        else:
            frame_indices = list(range(current_length))
        """
        Selects a 5 frame sequence from the dataset. Sets 4 for training and the last one
        as a target.
        """
        num_frames = self.dataset[idx]["frame_count"]
        frames = self.dataset[idx]["images"]
        self.image_attributes = parse_gdi_text(self.dataset[idx]["story"])

        frame_tensors = []
        description_list = []
        objects_list = []
        actions_list = []
        locations_list = []

        for frame_idx in range(4):
            # Process image
            image = FT.equalize(frames[frame_idx])
            input_frame = self.transform(image)
            frame_tensors.append(input_frame)

            # Process description
            description = self.image_attributes[frame_idx]["description"]
            input_ids = self.tokenizer(
                description,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=120
            ).input_ids
            description_list.append(input_ids.squeeze(0))

            # =========== ENHANCEMENT 7: Semantic tag utilization (objects, locations, actions from parsing/NER)===========
            objects = self.image_attributes[frame_idx]["objects"]
            actions = self.image_attributes[frame_idx]["actions"]
            locations = self.image_attributes[frame_idx]["locations"]

            # Convert tags to indices
            obj_indices = [self._get_tag_idx('objects', obj) for obj in objects[:5]]  # Max 5 objects
            act_indices = [self._get_tag_idx('actions', act) for act in actions[:3]]   # Max 3 actions
            loc_indices = [self._get_tag_idx('locations', loc) for loc in locations[:2]] # Max 2 locations

            # Pad to fixed length
            obj_indices = obj_indices + [0] * (5 - len(obj_indices))
            act_indices = act_indices + [0] * (3 - len(act_indices))
            loc_indices = loc_indices + [0] * (2 - len(loc_indices))

            objects_list.append(torch.tensor(obj_indices, dtype=torch.long))
            actions_list.append(torch.tensor(act_indices, dtype=torch.long))
            locations_list.append(torch.tensor(loc_indices, dtype=torch.long))
            # =========== END TAG PROCESSING ===========

        # Process target frame
        image_target = FT.equalize(frames[4])
        image_target = self.transform(image_target)

        # Process target description
        text_target_description = self.image_attributes[4]["description"]
        target_ids = self.tokenizer(
            text_target_description,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=120
        ).input_ids
        target_ids = target_ids.flatten()

        # Stack tensors
        sequence_tensor = torch.stack(frame_tensors)  # shape: (num_frames, C, H, W)
        description_tensor = torch.stack(description_list)  # (num_frames, max_length)
        objects_tensor = torch.stack(objects_list)  # (4, 5)
        actions_tensor = torch.stack(actions_list)   # (4, 3)
        locations_tensor = torch.stack(locations_list)  # (4, 2)

        # =========== PROCESS TARGET TAGS (for evaluation) ===========
        target_objects = self.image_attributes[4]["objects"]
        target_actions = self.image_attributes[4]["actions"]
        target_locations = self.image_attributes[4]["locations"]

        target_obj_indices = [self._get_tag_idx('objects', obj) for obj in target_objects[:5]]
        target_act_indices = [self._get_tag_idx('actions', act) for act in target_actions[:3]]
        target_loc_indices = [self._get_tag_idx('locations', loc) for loc in target_locations[:2]]

        # Pad target tags
        target_obj_indices = target_obj_indices + [0] * (5 - len(target_obj_indices))
        target_act_indices = target_act_indices + [0] * (3 - len(target_act_indices))
        target_loc_indices = target_loc_indices + [0] * (2 - len(target_loc_indices))

        target_objects_tensor = torch.tensor(target_obj_indices, dtype=torch.long)
        target_actions_tensor = torch.tensor(target_act_indices, dtype=torch.long)
        target_locations_tensor = torch.tensor(target_loc_indices, dtype=torch.long)
        # =========== END TARGET TAGS ===========

        return (
            sequence_tensor,           # Input images (4, C, H, W)
            description_tensor,        # Input descriptions (4, max_length)
            objects_tensor,            # Input objects (4, 5)
            actions_tensor,            # Input actions (4, 3)
            locations_tensor,          # Input locations (4, 2)
            image_target,              # Target image (C, H, W)
            target_ids,                # Target text (max_length)
            target_objects_tensor,     # Target objects (5,)
            target_actions_tensor,     # Target actions (3,)
            target_locations_tensor    # Target locations (2,)
        )

    def get_vocab_sizes(self):
        """Return vocabulary sizes for tag embeddings"""
        return {
            'objects': len(self.tag_vocab['objects']) + 1,  # +1 for padding (0)
            'actions': len(self.tag_vocab['actions']) + 1,
            'locations': len(self.tag_vocab['locations']) + 1
        }

"""We will use text autoencoding (reconstructing the same text) to develop representations of the text (I provide some existing weights for this, but you can train your own)"""

# @title Text task dataset (text autoencoding)
class TextTaskDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
      num_frames = self.dataset[idx]["frame_count"]
      self.image_attributes = parse_gdi_text(self.dataset[idx]["story"])

      # Pick
      frame_idx = np.random.randint(0, 5)
      description = self.image_attributes[frame_idx]["description"]

      return description  # Returning the whole description

# @title Dataset for image autoencoder task
class AutoEncoderTaskDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset
        self.transform = transforms.Compose([
          transforms.Resize((240, 500)),# Reasonable size based on our previous analysis
          transforms.ToTensor(), # HxWxC -> CxHxW
        ])

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
      num_frames = self.dataset[idx]["frame_count"]
      frames = self.dataset[idx]["images"]

      # Pick a frame at random
      frame_idx = torch.randint(0, num_frames-1, (1,)).item()
      input_frame = self.transform(frames[frame_idx]) # Input to the autoencoder

      return input_frame, # Returning the image

"""## 1.3 Creating and testing our dataset objects and loaders


---


"""

# @title For the Sequence prediction task
tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased",  padding=True, truncation=True)
sp_train_dataset = SequencePredictionDataset(train_dataset, tokenizer) # Instantiate the train dataset
sp_test_dataset = SequencePredictionDataset(test_dataset, tokenizer, curriculum_step=3) # Set to max curriculum step

# Let's do things properly, we will also have a validation split
# Split the training dataset into training and validation sets

# =========== ENHANCEMENT 8: 80/10/10 data split at video/story level ===========
# Group by story/video (simplified - assuming each index is a different story)
story_ids = list(range(len(sp_train_dataset)))
np.random.shuffle(story_ids)
train_size = int(0.8 * len(story_ids))
val_size = int(0.1 * len(story_ids))
test_size = len(story_ids) - train_size - val_size

train_ids = story_ids[:train_size]
val_ids = story_ids[train_size:train_size+val_size]
test_ids = story_ids[train_size+val_size:]

train_subset = torch.utils.data.Subset(sp_train_dataset, train_ids)
val_subset = torch.utils.data.Subset(sp_train_dataset, val_ids)
test_subset = torch.utils.data.Subset(sp_train_dataset, test_ids)

# Instantiate the dataloaders
train_dataloader = DataLoader(train_subset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_subset, batch_size=8, shuffle=True)
test_dataloader = DataLoader(sp_test_dataset, batch_size=4, shuffle=False)

# @title For the text task
tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased",  padding=True, truncation=True)
text_dataset = TextTaskDataset(train_dataset)
text_dataloader = DataLoader(text_dataset, batch_size=4, shuffle=True)

# @title For the image autoencoder task
autoencoder_dataset = AutoEncoderTaskDataset(train_dataset)
autoencoder_dataloader = DataLoader(autoencoder_dataset, batch_size=4, shuffle=True)

# @title Testing some of the outputs of the SP dataset
# CORRECT - Match the new 10-element output
(frames, descriptions, objects_seq, actions_seq, locations_seq,
 image_target, text_target, target_objects, target_actions, target_locations) = sp_train_dataset[np.random.randint(0,400)]

print("Description: ", descriptions.shape)
figure, ax = plt.subplots(1,1)
show_image(ax, image_target)

# Do some tests on the batches (try with batch size small)
# CORRECT - Add closing parenthesis and semicolon
(frames, descriptions, objects_seq, actions_seq, locations_seq,
 image_target, text_target, target_objects, target_actions, target_locations) = next(iter(train_dataloader))
print(frames.shape)
print(descriptions.shape)

"""# **Models**


---

## 2.1 The NLP models
"""

# @title The text autoencoder (Seq2Seq)

class EncoderLSTM(nn.Module):
    """
      Encodes a sequence of tokens into a latent space representation.
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                            batch_first=True, dropout=dropout if num_layers > 1 else 0)

    def forward(self, input_seq):
        embedded = self.embedding(input_seq)
        outputs, (hidden, cell) = self.lstm(embedded)
        return outputs, hidden, cell

class DecoderLSTM(nn.Module):
    """
      Decodes a latent space representation into a sequence of tokens.
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                            batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim

    def forward(self, input_seq, hidden, cell):
        embedded = self.embedding(input_seq)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.out(output)
        return prediction, hidden, cell

# We create the basic text autoencoder (a special case of a sequence to sequence model)
class Seq2SeqLSTM(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, input_seq, target_seq):
        # input_seq and target_seq are both your 'input_ids'
        # 1. Encode the input sequence
        _enc_out, hidden, cell = self.encoder(input_seq)

        # 2. Create the "shifted" decoder input for teacher forcing.
        # We want to predict target_seq[:, 1:]
        # So, we feed in target_seq[:, :-1]
        # (i.e., feed "[SOS], hello, world" to predict "hello, world, [EOS]")
        decoder_input = target_seq[:, :-1]

        # 3. Run the decoder *once* on the entire sequence.
        # It takes the encoder's final state (hidden, cell)
        # and the full "teacher" sequence (decoder_input).
        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)

        # predictions shape will be (batch_size, seq_len-1, vocab_size)
        return predictions

def generate(model, hidden, cell, max_len, sos_token_id, eos_token_id, device=None):
    """
    This function generates a sequence of tokens using the provided decoder.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Ensure the model is in evaluation mode
    model.eval()

    # Get batch size from hidden state
    batch_size = hidden.size(1)

    # Start with the SOS token for each sequence in batch
    dec_input = torch.full((batch_size, 1), sos_token_id, dtype=torch.long, device=device)

    generated_tokens_list = []

    # AUTOREGRESSIVE LOOP
    for _ in range(max_len):
        with torch.no_grad():
            # Run the decoder one step at a time
            prediction, hidden, cell = model(dec_input, hidden, cell)

        # Get next token (greedy decoding - you can change to sampling later)
        logits = prediction[:, -1, :]  # Take the last timestep
        temperature = 0.9
        probabilities = torch.softmax(logits / temperature, dim=-1)
        next_token = torch.multinomial(probabilities, num_samples=1)

        # Check for EOS tokens
        for i in range(batch_size):
            token_id = next_token[i].item()
            if token_id == eos_token_id:
                # Replace EOS with padding to stop generation
                next_token[i] = eos_token_id

        # The predicted token becomes the input for the next iteration
        dec_input = next_token

        # Store tokens
        generated_tokens_list.append(next_token.squeeze(1).cpu())

    # Stack tokens: [batch_size, max_len]
    if generated_tokens_list:
        all_tokens = torch.stack(generated_tokens_list, dim=1)
    else:
        all_tokens = torch.empty(batch_size, 0, dtype=torch.long)

    return all_tokens

"""## 2.2 The Vision models"""

import clip

class CLIPEvaluator:
    def __init__(self, device):
        self.device = device
        self.model, self.preprocess = clip.load("ViT-B/32", device=device)
        self.model.eval()

    def compute_score(self, images, texts):
        """Compute CLIP similarity between images and texts"""
        if images.dim() == 4:  # (batch, C, H, W)
            images = images
        else:
            images = images.unsqueeze(0)

        # Preprocess images if needed
        if images.max() > 1.0:
            images = images / 255.0

        with torch.no_grad():
            # Encode images and texts
            image_features = self.model.encode_image(images)
            text_tokens = clip.tokenize(texts).to(self.device)
            text_features = self.model.encode_text(text_tokens)

            # Normalize features
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)

            # Compute cosine similarity
            similarity = (image_features @ text_features.T).diag()

        return similarity.mean().item()

    def compute_image_retrieval(self, query_images, candidate_images, top_k=5):
        """Image-to-image retrieval"""
        with torch.no_grad():
            query_features = self.model.encode_image(query_images)
            candidate_features = self.model.encode_image(candidate_images)

            # Normalize
            query_features = query_features / query_features.norm(dim=-1, keepdim=True)
            candidate_features = candidate_features / candidate_features.norm(dim=-1, keepdim=True)

            # Compute similarity matrix
            similarity = query_features @ candidate_features.T

            # Get top-k matches
            top_k_scores, top_k_indices = similarity.topk(top_k, dim=1)

        return top_k_scores, top_k_indices


class ImprovedBackbone(nn.Module):
    """
    Better convolutional backbone with batch normalization
    """
    def __init__(self, latent_dim=256):
        super(ImprovedBackbone, self).__init__()
        # Encoder with more capacity
        self.encoder = nn.Sequential(
            # Layer 1: 60x125 -> 30x62
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),

            # Layer 2: 30x62 -> 15x31
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            # Layer 3: 15x31 -> 7x15
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            # Layer 4: 7x15 -> 3x7
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),

            # Flatten and project to latent space
            nn.Flatten(),
            nn.Linear(512 * 3 * 7, latent_dim), # Corrected dimension from 512 * 4 * 8
            nn.BatchNorm1d(latent_dim),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Calculate the flattened dimension
        self.flatten_dim = 512 * 3 * 7 # Corrected dimension from 512 * 4 * 8

    def forward(self, x):
        return self.encoder(x)

class ImprovedVisualEncoder(nn.Module):
    """
    Encoder with residual connections
    """
    def __init__(self, latent_dim=256):
        super(ImprovedVisualEncoder, self).__init__()
        self.backbone = ImprovedBackbone(latent_dim)
        self.projection = nn.Sequential(
            nn.Linear(latent_dim, latent_dim // 2),
            nn.BatchNorm1d(latent_dim // 2),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        features = self.backbone(x)
        compressed = self.projection(features)
        return compressed

# =========== ENHANCEMENT 3: LATENT DIFFUSION DECODER ===========
class ViewLayer(nn.Module):
    """Helper layer to reshape tensors"""
    def __init__(self, *shape):
        super().__init__()
        self.shape = shape

    def forward(self, x):
        return x.view(x.size(0), *self.shape)

class DiffusionDecoder(nn.Module):
    """
    Latent Diffusion Decoder with cross-attention for text conditioning
    """
    def __init__(self, latent_dim=128, diffusion_steps=100, num_cross_heads=8):
        super().__init__()
        self.decoder_latent_dim = latent_dim # Renaming for clarity: this is the dimension diffusion happens in
        self.diffusion_steps = diffusion_steps

        # Time embedding
        self.time_embed = nn.Sequential(
            nn.Linear(1, 128),
            nn.SiLU(),
            nn.Linear(128, self.decoder_latent_dim) # Output time embedding in decoder_latent_dim
        )

        # Cross-attention for text conditioning
        self.cross_attn = nn.MultiheadAttention(self.decoder_latent_dim, num_cross_heads, batch_first=True)
        self.cross_attn_norm = nn.LayerNorm(self.decoder_latent_dim)

        # Noise prediction network (predicts noise in the latent space)
        # Input: [noisy_x_t (latent) + t_embed (latent)] -> 2 * decoder_latent_dim
        # Output: predicted_noise_epsilon (latent) -> decoder_latent_dim
        self.noise_predictor = nn.Sequential(
            nn.Linear(self.decoder_latent_dim * 2, self.decoder_latent_dim * 4),
            nn.SiLU(),
            nn.Linear(self.decoder_latent_dim * 4, self.decoder_latent_dim * 2),
            nn.SiLU(),
            nn.Linear(self.decoder_latent_dim * 2, self.decoder_latent_dim)
        )

        # Separate module to decode the final denoised latent vector into an image
        self.latent_to_image_decoder = nn.Sequential(
            nn.Linear(self.decoder_latent_dim, 512 * 3 * 7),
            nn.BatchNorm1d(512 * 3 * 7),
            nn.ReLU(inplace=True),
            ViewLayer(512, 3, 7),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, output_padding=(1, 1)), # -> (256, 7, 15)
            nn.GroupNorm(8, 256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, output_padding=(1, 1)), # -> (128, 15, 31)
            nn.GroupNorm(8, 128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, output_padding=(0, 0)), # -> (64, 30, 62)
            nn.GroupNorm(8, 64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, output_padding=(0, 1)), # -> (32, 60, 125)
            nn.GroupNorm(8, 32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 3, kernel_size=3, padding=1),
            nn.Tanh() # Assuming normalized images between -1 and 1
        )

        # Noise schedule
        self.register_buffer('betas', self.get_beta_schedule())
        self.register_buffer('alphas', 1. - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - self.alphas_cumprod))

    def get_beta_schedule(self):
        """Cosine noise schedule - better than linear"""
        steps = self.diffusion_steps
        s = 0.008
        t = torch.arange(steps + 1) / steps
        f_t = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2
        alphas_cumprod = f_t / f_t[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0.0001, 0.9999)

    def forward(self, z_latent, text_features=None, return_noise=False):
        batch_size = z_latent.size(0)

        # For training, we sample a random timestep and add noise to the *clean target latent* (z_latent)
        noise_epsilon = torch.randn_like(z_latent) # This is the target noise epsilon
        t = torch.randint(0, self.diffusion_steps, (batch_size,), device=z_latent.device).long()

        sqrt_alpha_t_cumprod = self.sqrt_alphas_cumprod[t].view(-1, 1)
        sqrt_one_minus_alpha_t_cumprod = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)

        # Create noisy latent (diffusion forward process)
        # x_t = sqrt(alpha_cumprod_t) * x_0 + sqrt(1 - alpha_cumprod_t) * epsilon
        noisy_x_t = sqrt_alpha_t_cumprod * z_latent + sqrt_one_minus_alpha_t_cumprod * noise_epsilon

        # Time embedding
        t_embed = self.time_embed((t.float() / self.diffusion_steps).unsqueeze(-1))

        # Cross-attention with text features if provided
        if text_features is not None and text_features.dim() > 1:
            if text_features.dim() == 2:
                text_features = text_features.unsqueeze(1)
            attn_out, _ = self.cross_attn(
                noisy_x_t.unsqueeze(1),
                text_features,
                text_features
            )
            attn_out = self.cross_attn_norm(noisy_x_t.unsqueeze(1) + attn_out)
            noisy_x_t_attended = attn_out.squeeze(1)
        else:
            noisy_x_t_attended = noisy_x_t

        # Predict noise (in latent space)
        combined_input_to_noise_predictor = torch.cat([noisy_x_t_attended, t_embed], dim=-1)
        predicted_noise_epsilon = self.noise_predictor(combined_input_to_noise_predictor)

        if return_noise:
            # Estimate x_0 from current noisy_x_t and predicted_noise_epsilon
            estimated_x_0 = (noisy_x_t - sqrt_one_minus_alpha_t_cumprod * predicted_noise_epsilon) / sqrt_alpha_t_cumprod
            decoded_image_for_metrics = self.latent_to_image_decoder(estimated_x_0)
            return decoded_image_for_metrics, predicted_noise_epsilon # Returns image for recon loss/metrics, latent for noise loss
        else:
            # This path maps the *input* z_latent (which is assumed to be clean) directly to an image
            return self.latent_to_image_decoder(z_latent)

    @torch.no_grad()
    def sample(self, conditioning_latent, text_features=None, num_steps=None):
        """
        Sampling from the diffusion model (denoising process)
        conditioning_latent: [batch, encoder_latent_dim] - used for batch size and device. NOT the initial noisy latent.
        """
        if num_steps is None:
            num_steps = self.diffusion_steps

        batch_size = conditioning_latent.size(0)

        # Start from random noise in the latent space (self.decoder_latent_dim)
        x_t = torch.randn(batch_size, self.decoder_latent_dim, device=conditioning_latent.device)

        # Reverse diffusion process
        for i in reversed(range(num_steps)):
            t = torch.full((batch_size,), i, device=x_t.device).long()

            # Time embedding
            t_embed = self.time_embed((t.float() / self.diffusion_steps).unsqueeze(-1))

            # Apply cross-attention if text features provided
            x_t_input_to_noise_predictor = x_t
            if text_features is not None and text_features.dim() > 1:
                if text_features.dim() == 2:
                    text_features = text_features.unsqueeze(1)
                attn_out, _ = self.cross_attn(
                    x_t.unsqueeze(1),
                    text_features,
                    text_features
                )
                attn_out = self.cross_attn_norm(x_t.unsqueeze(1) + attn_out)
                x_t_input_to_noise_predictor = attn_out.squeeze(1)

            # Predict noise (in latent space) from noisy latent x_t and time t
            combined = torch.cat([x_t_input_to_noise_predictor, t_embed], dim=-1)
            predicted_noise_epsilon = self.noise_predictor(combined)

            # Get coefficients for current time step t
            alpha_t = self.alphas[t].view(-1, 1)
            beta_t = self.betas[t].view(-1, 1)
            alpha_cumprod_t = self.alphas_cumprod[t].view(-1, 1)
            sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)

            # Calculate mean for x_{t-1} based on predicted noise
            mean = (x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * predicted_noise_epsilon) / torch.sqrt(alpha_t)

            if i > 0:
                # Add noise for steps t > 0
                noise_z = torch.randn_like(x_t)
                x_t = mean + torch.sqrt(beta_t) * noise_z
            else:
                # For the last step (t=0), no noise is added
                x_t = mean

        # After the loop, x_t is the fully denoised latent (estimated x_0)
        output_image = self.latent_to_image_decoder(x_t)

        return output_image

class ImprovedVisualAutoencoder(nn.Module):
    def __init__(self, latent_dim=256, use_diffusion=True):
        super(ImprovedVisualAutoencoder, self).__init__()
        self.encoder = ImprovedVisualEncoder(latent_dim)
        if use_diffusion:
            # Use diffusion decoder with half the latent dim (encoder compresses to latent_dim//2)
            self.decoder = DiffusionDecoder(latent_dim=latent_dim//2, diffusion_steps=50) # latent_dim here is self.decoder_latent_dim in DiffusionDecoder
        else:
            # Keep the old decoder as fallback and fix its dimensions
            class ImprovedVisualDecoder(nn.Module):
                """Original decoder as fallback"""
                def __init__(self, latent_dim):
                    super().__init__()
                    self.fc = nn.Sequential(
                        nn.Linear(latent_dim//2, 512 * 3 * 7),
                        nn.BatchNorm1d(512 * 3 * 7),
                        nn.ReLU()
                    )
                    self.decoder = nn.Sequential(
                        # Adjusted output_padding and final Conv2d kernel/padding for 60x125 output
                        nn.ConvTranspose2d(512, 256, 4, 2, 1, output_padding=(1, 1)),
                        nn.BatchNorm2d(256), nn.ReLU(),
                        nn.ConvTranspose2d(256, 128, 4, 2, 1, output_padding=(1, 1)),
                        nn.BatchNorm2d(128), nn.ReLU(),
                        nn.ConvTranspose2d(128, 64, 4, 2, 1, output_padding=(0, 0)),
                        nn.BatchNorm2d(64), nn.ReLU(),
                        nn.ConvTranspose2d(64, 32, 4, 2, 1, output_padding=(0, 1)),
                        nn.BatchNorm2d(32), nn.ReLU(),
                        nn.Conv2d(32, 3, kernel_size=3, padding=1), # Changed kernel and padding
                        nn.Sigmoid()
                    )

                def forward(self, z):
                    x = self.fc(z)
                    x = x.view(x.size(0), 512, 3, 7)
                    x = self.decoder(x)
                    return x[:, :, :60, :125]

            self.decoder = ImprovedVisualDecoder(latent_dim)

    def forward(self, x, text_features=None):
        z = self.encoder(x) # z is (batch, latent_dim//2)
        if isinstance(self.decoder, DiffusionDecoder):
            # For diffusion decoder during training
            # The `pred_image_content` returned here is the decoded image from estimated x_0,
            # and `predicted_noise` is the noise predicted in latent space.
            return self.decoder(z, text_features=text_features, return_noise=True) # Ensure this returns image AND noise for the training loop
        else:
            return self.decoder(z)

"""## 2.3 The main architecture

"""

# =========== ENHANCEMENT 1: CROSS-MODAL TEMPORAL TRANSFORMER ===========
class CrossModalTemporalTransformer(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_layers=4):
        super().__init__()
        self.d_model = d_model

        # Intra-modal temporal self-attention
        self.visual_self_attn = nn.MultiheadAttention(d_model, nhead)
        self.text_self_attn = nn.MultiheadAttention(d_model, nhead)

        # Cross-modal cross-attention
        self.visual_cross_attn = nn.MultiheadAttention(d_model, nhead)
        self.text_cross_attn = nn.MultiheadAttention(d_model, nhead)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, visual_seq, text_seq):
        # visual_seq: [seq_len, batch, d_model]
        # text_seq: [seq_len, batch, d_model]

        # Intra-modal self-attention
        visual_self, _ = self.visual_self_attn(visual_seq, visual_seq, visual_seq)
        text_self, _ = self.text_self_attn(text_seq, text_seq, text_seq)

        # Add & Norm
        visual_self = self.norm1(visual_seq + visual_self)
        text_self = self.norm1(text_seq + text_self)

        # Cross-modal attention
        visual_cross, _ = self.visual_cross_attn(visual_self, text_self, text_self)
        text_cross, _ = self.text_cross_attn(text_self, visual_self, visual_self)

        # Add & Norm
        visual_out = self.norm2(visual_self + visual_cross)
        text_out = self.norm2(text_self + text_cross)

        return visual_out, text_out

# Keep the original Attention class for backward compatibility
class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_dim, 1)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, rnn_outputs):
        energy = self.attn(rnn_outputs).squeeze(2)
        attn_weights = self.softmax(energy)
        context = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs)
        return context.squeeze(1)

# =========== ENHANCEMENT 4: ENHANCED SEQUENCEPREDICTOR WITH CMTT AND TAGS ===========
class EnhancedSequencePredictor(nn.Module):
    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim, gru_hidden_dim, tag_vocab_sizes=None):
        super().__init__()

        # Existing encoders
        self.image_encoder = visual_autoencoder.encoder
        self.text_encoder = text_autoencoder.encoder

        # Projections for CMTT
        self.visual_proj = nn.Linear(visual_autoencoder.encoder.projection[0].out_features, 512)
        self.text_proj = nn.Linear(text_autoencoder.encoder.hidden_dim, 512)

        # CMTT Enhancement
        self.cmtt = CrossModalTemporalTransformer(d_model=512, nhead=8, num_layers=2)

        # =========== TAG EMBEDDINGS ===========
        if tag_vocab_sizes is None:
            tag_vocab_sizes = {'objects': 1000, 'actions': 500, 'locations': 200}

        self.object_embedding = nn.Embedding(tag_vocab_sizes['objects'], 32, padding_idx=0)
        self.action_embedding = nn.Embedding(tag_vocab_sizes['actions'], 32, padding_idx=0)
        self.location_embedding = nn.Embedding(tag_vocab_sizes['locations'], 32, padding_idx=0)

        # Tag processing layers
        self.tag_processor = nn.Sequential(
            nn.Linear(32*3, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128)
        )

        # =========== ADD TAG PREDICTION HEAD ===========
        self.tag_pred_head = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, tag_vocab_sizes['objects'] + tag_vocab_sizes['actions'] + tag_vocab_sizes['locations'])
        )
        # =========== END TAG EMBEDDINGS ===========

        # Fusion layer after CMTT (now includes tag features)
        self.fusion = nn.Linear(512 * 2 + 128, latent_dim)

        # Keep existing components
        self.temporal_rnn = nn.GRU(latent_dim, gru_hidden_dim, batch_first=True)
        self.attention = Attention(gru_hidden_dim)
        self.projection = nn.Sequential(
            nn.Linear(gru_hidden_dim * 2, latent_dim),
            nn.ReLU()
        )

        # Decoders
        self.image_decoder = visual_autoencoder.decoder
        self.text_decoder = text_autoencoder.decoder

        # The text decoder's LSTM hidden_dim is 16
        self.fused_to_h0 = nn.Linear(latent_dim, text_autoencoder.decoder.hidden_dim)
        self.fused_to_c0 = nn.Linear(latent_dim, text_autoencoder.decoder.hidden_dim)

    def forward(self, image_seq, text_seq, objects_seq, actions_seq, locations_seq, target_seq):
        batch_size, seq_len, C, H, W = image_seq.shape

        # Encode sequences
        img_flat = image_seq.view(batch_size * seq_len, C, H, W)
        txt_flat = text_seq.view(batch_size * seq_len, -1)

        z_v_flat = self.image_encoder(img_flat)
        _, hidden_text_encoder, cell_text_encoder = self.text_encoder(txt_flat)

        # =========== PROCESS TAGS ===========
        # Reshape tags: (batch, seq_len, num_tags) -> (batch*seq_len, num_tags)
        objects_flat = objects_seq.view(batch_size * seq_len, -1)
        actions_flat = actions_seq.view(batch_size * seq_len, -1)
        locations_flat = locations_seq.view(batch_size * seq_len, -1)

        # Get embeddings for each tag (handle multiple tags per frame)
        # For objects: (batch*seq_len, 5) -> average embedding
        obj_emb = self.object_embedding(objects_flat)  # (batch*seq_len, 5, 32)
        obj_emb = obj_emb.mean(dim=1)  # Average over 5 objects

        # For actions: (batch*seq_len, 3) -> average embedding
        act_emb = self.action_embedding(actions_flat)  # (batch*seq_len, 3, 32)
        act_emb = act_emb.mean(dim=1)

        # For locations: (batch*seq_len, 2) -> average embedding
        loc_emb = self.location_embedding(locations_flat)  # (batch*seq_len, 2, 32)
        loc_emb = loc_emb.mean(dim=1)

        # Combine tag embeddings
        tag_emb = torch.cat([obj_emb, act_emb, loc_emb], dim=-1)  # (batch*seq_len, 96)
        tag_features = self.tag_processor(tag_emb)  # (batch*seq_len, 128)
        # =========== END TAG PROCESSING ===========

        # Project for CMTT
        z_v_proj = self.visual_proj(z_v_flat)
        z_t_proj = self.text_proj(hidden_text_encoder.squeeze(0))

        # Reshape for temporal processing
        z_v_seq = z_v_proj.view(batch_size, seq_len, -1).transpose(0, 1)
        z_t_seq = z_t_proj.view(batch_size, seq_len, -1).transpose(0, 1)
        tag_seq = tag_features.view(batch_size, seq_len, -1).transpose(0, 1)

        # Fuse modalities (including tags) - THIS IS THE CORRECT IMPLEMENTATION
        z_v_cmtt, z_t_cmtt = self.cmtt(z_v_seq, z_t_seq)
        z_v_fused = z_v_cmtt.mean(dim=0)
        z_t_fused = z_t_cmtt.mean(dim=0)
        tag_fused = tag_seq.mean(dim=0)

        z_fusion = torch.cat([z_v_fused, z_t_fused, tag_fused], dim=-1)
        z_final = self.fusion(z_fusion)

        # Continue with existing flow
        zseq, h = self.temporal_rnn(z_final.unsqueeze(1))
        h = h.squeeze(0)
        context = self.attention(zseq)
        z = self.projection(torch.cat((h, context), dim=1))

        # Decode
        pred_image_content = self.image_decoder(z)

        # Prepare initial hidden and cell states for text decoder
        h0 = self.fused_to_h0(z).unsqueeze(0)
        c0 = self.fused_to_c0(z).unsqueeze(0)

        decoder_input = target_seq[:, :-1]
        predicted_text_logits_k, _hidden, _cell = self.text_decoder(decoder_input, h0, c0)

        # =========== ADD TAG PREDICTIONS ===========
        tag_logits = self.tag_pred_head(tag_fused)
        # =========== END TAG PREDICTIONS ===========

        return pred_image_content, z, predicted_text_logits_k, z_final, tag_fused, tag_logits

"""# **Training**


---



"""

# @title Training utility functions: To initialize and to visualize the progress


def init_weights(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Linear):
        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
        nn.init.constant_(m.bias, 0)

# new addition
# =========== ENHANCEMENT 6: COMPREHENSIVE EVALUATION ===========
class ComprehensiveEvaluator:
    def __init__(self):
        # Get the device from the global scope (assuming 'device' is defined globally)
        global device
        if 'device' not in globals():
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        try:
            self.bleu = BLEUScore().to(device)
            self.rouge = ROUGEScore().to(device)
            self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
        except:
            print("Warning: torchmetrics not available or device transfer failed, using dummy metrics")
            self.bleu = None
            self.rouge = None
            self.ssim = None

    def compute_metrics(self, pred_images, target_images, pred_texts, target_texts):
        metrics = {}

        # Image metrics
        metrics['mse'] = F.mse_loss(pred_images, target_images).item()

        if self.ssim:
            metrics['ssim'] = self.ssim(pred_images, target_images).item()
        else:
            metrics['ssim'] = 0.0

        # Text metrics (simplified - skip if torchmetrics not available)
        try:
            if self.bleu and self.rouge:
                # pred_texts and target_texts are lists of strings
                metrics['bleu'] = self.bleu(pred_texts, [[t] for t in target_texts]).item()
                rouge_scores = self.rouge(pred_texts, target_texts)
                metrics['rouge'] = rouge_scores['rouge1_fmeasure'].item()
            else:
                metrics['bleu'] = 0.0
                metrics['rouge'] = 0.0
        except Exception as e:
            print(f"Text metrics error: {e}")
            metrics['bleu'] = 0.0
            metrics['rouge'] = 0.0

        return metrics

# Create evaluator instance - ADD THIS RIGHT AFTER THE CLASS
evaluator = ComprehensiveEvaluator()


# Plots four images and their reconstructions
def validation(model, data_loader):
    model.eval()
    with torch.no_grad():
        # Unpack all elements including tags
        frames, descriptions, objects_seq, actions_seq, locations_seq, \
        image_target, text_target, target_objects, target_actions, target_locations = next(iter(data_loader))

        descriptions = descriptions.to(device)
        frames = frames.to(device)
        objects_seq = objects_seq.to(device)
        actions_seq = actions_seq.to(device)
        locations_seq = locations_seq.to(device)
        image_target = image_target.to(device)
        text_target = text_target.to(device)

        # Forward pass with tags - ALWAYS get z_final
        pred_image_content, pred_image_context, predicted_text_logits_k, z_final, tag_features, tag_logits = model(
            frames, descriptions, objects_seq, actions_seq, locations_seq, text_target
        )

        # If using diffusion decoder, replace the image prediction
        if isinstance(model.image_decoder, DiffusionDecoder):
            # For diffusion decoder during inference, use sample method
            pred_image_content = model.image_decoder.sample(z_final)

        # Reconstruct hidden and cell from z_final for text decoding
        hidden = model.fused_to_h0(z_final).unsqueeze(0)
        cell = model.fused_to_c0(z_final).unsqueeze(0)

        # Generate text predictions for metrics
        generated_texts = []
        for i in range(min(4, frames.size(0))):  # For first 4 samples
            sample_hidden = hidden[:, i, :].unsqueeze(1)
            sample_cell = cell[:, i, :].unsqueeze(1)
            gen_tokens = generate(model.text_decoder,
                         sample_hidden,
                         sample_cell,
                         max_len=50,
                         sos_token_id=tokenizer.cls_token_id,
                         eos_token_id=tokenizer.sep_token_id,
                         device=device)
            generated_texts.append(tokenizer.decode(gen_tokens.squeeze(0), skip_special_tokens=True))

        # Convert target texts
        target_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in text_target[:4]]

        # Compute metrics
        try:
            metrics = evaluator.compute_metrics(
                pred_image_content[:4],
                image_target[:4],
                generated_texts,
                target_texts
            )
            print(f"Validation Metrics: {metrics}")

            # Track metrics for validation
            if 'metrics_tracker' in globals():
                metrics_tracker.update('val', 'ssim', metrics.get('ssim', 0))
                metrics_tracker.update('val', 'bleu', metrics.get('bleu', 0))
                metrics_tracker.update('val', 'rouge', metrics.get('rouge', 0))
        except Exception as e:
            print(f"Could not compute metrics: {e}")
            if 'metrics_tracker' in globals():
                metrics_tracker.update('val', 'ssim', 0)
                metrics_tracker.update('val', 'bleu', 0)
                metrics_tracker.update('val', 'rouge', 0)
# =========== ADD CLIP SCORE ===========
        try:
            clip_score = clip_evaluator.compute_score(
                pred_image_content[:4],
                generated_texts
            )
            print(f"CLIP Score: {clip_score:.4f}")
        except Exception as e:
            print(f"CLIP score error: {e}")
        # =========== END CLIP SCORE ===========
        # =========== VISUALIZATION ===========
        figure, ax = plt.subplots(2, 6, figsize=(20, 5), gridspec_kw={'height_ratios': [2, 1.5]})

        for i in range(4):
            im = frames[0, i, :, :, :].cpu()
            show_image(ax[0,i], im)
            ax[0,i].set_aspect('auto')
            ax[0,i].axis('off')

            # Display tags alongside description
            obj_tags = [k for k, v in sp_train_dataset.tag_vocab['objects'].items()
                       if v == objects_seq[0, i, 0].item() and v != 0]
            act_tags = [k for k, v in sp_train_dataset.tag_vocab['actions'].items()
                       if v == actions_seq[0, i, 0].item() and v != 0]
            loc_tags = [k for k, v in sp_train_dataset.tag_vocab['locations'].items()
                       if v == locations_seq[0, i, 0].item() and v != 0]

            desc_text = tokenizer.decode(descriptions[0, i, :], skip_special_tokens=True)
            tag_text = f"Tags: {', '.join(obj_tags[:1] + act_tags[:1] + loc_tags[:1])}"
            full_text = f"{desc_text}\n{tag_text}"

            wrapped_text = textwrap.fill(full_text, width=40)
            ax[1,i].text(0.5, 0.99, wrapped_text, ha='center', va='top', fontsize=10, wrap=True)
            ax[1,i].axis('off')

        # Show target
        show_image(ax[0,4], image_target[0].cpu())
        ax[0,4].set_title('Target')
        ax[0,4].set_aspect('auto')
        ax[0,4].axis('off')

        text_target_to_decode = text_target[0].squeeze()
        wrapped_text = textwrap.fill(tokenizer.decode(text_target_to_decode, skip_special_tokens=True), width=40)
        ax[1,4].text(0.5, 0.99, wrapped_text, ha='center', va='top', fontsize=10, wrap=False)
        ax[1,4].axis('off')

        # Show prediction
        output = pred_image_content[0].cpu()
        show_image(ax[0,5], output)
        ax[0,5].set_title('Predicted')
        ax[0,5].set_aspect('auto')
        ax[0,5].axis('off')

        # Generate and show predicted text
        single_sample_hidden = hidden[:, 0, :].unsqueeze(1)
        single_sample_cell = cell[:, 0, :].unsqueeze(1)
        generated_tokens = generate(
            model.text_decoder,
            single_sample_hidden,
            single_sample_cell,
            max_len=150,
            sos_token_id=tokenizer.cls_token_id,
            eos_token_id=tokenizer.sep_token_id,
            device=device
        )
        wrapped_text = textwrap.fill(tokenizer.decode(generated_tokens.squeeze(0)), width=40)
        ax[1,5].text(0.5, 0.99, wrapped_text, ha='center', va='top', fontsize=10, wrap=False)
        ax[1,5].axis('off')

        plt.tight_layout()
        plt.show()

"""## 3.1 Initialization and setup"""

# @title Variables and initial setup
torch.cuda.empty_cache()
gc.collect()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =========== ADD CLIP EVALUATOR ===========
clip_evaluator = CLIPEvaluator(device)
print("CLIP model loaded for evaluation")
# =========== END CLIP EVALUATOR ===========

N_EPOCHS = 20  #change to 20 for better training
emb_dim = 16
latent_dim = 16
num_layers = 1
dropout = True

# @title Initializing the NLP models
# 1) Mount Google Drive (run if you haven't already)
from google.colab import drive
drive.mount('/content/gdrive')

# 2) Create DL_Checkpoints folder if it doesn't exist
!mkdir -p "/content/gdrive/MyDrive/DL_Checkpoints"

# 3) Copy the file (handles the space and (2) in the name) and rename it to the exact filename expected
# Replace the source path below if your file is somewhere else (see notes after this block)
!cp "/content/gdrive/MyDrive/text_autoencoder (2).pth" "/content/gdrive/MyDrive/DL_Checkpoints/text_autoencoder.pth"

# 4) Confirm the file is now in the expected folder
!ls -l "/content/gdrive/MyDrive/DL_Checkpoints"
encoder = EncoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)
decoder = DecoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)
text_autoencoder = Seq2SeqLSTM(encoder, decoder).to(device)
text_autoencoder, _, _, _ = load_checkpoint_from_drive(text_autoencoder, None, filename='text_autoencoder.pth')

total_params = sum(p.numel() for p in text_autoencoder.parameters())
print(f"Total parameters (Not trainable): {total_params}")
# Deactivating training from this model for efficiency (although not ideal)
for param in text_autoencoder.parameters():
        param.requires_grad = False
text_autoencoder.train()
#/content/gdrive/MyDrive/text_autoencoder (2).pth

# @title Initializing visual models
visual_autoencoder = ImprovedVisualAutoencoder(latent_dim=256, use_diffusion=True)  # ADD use_diffusion=True
visual_autoencoder.apply(init_weights)

total_params = sum(p.numel() for p in visual_autoencoder.parameters() if p.requires_grad)
print(f"Total trainable parameters in visual autoencoder: {total_params}")

# @title Initialize the main architecture
# We put all the sizes the same, not ideal as well

# new change
tag_vocab_sizes = sp_train_dataset.get_vocab_sizes()
sequence_predictor = EnhancedSequencePredictor(
    visual_autoencoder,
    text_autoencoder,
    128,
    128,
    tag_vocab_sizes=tag_vocab_sizes
)
sequence_predictor.to(device)

# # Print number of trainable parameters
total_params = sum(p.numel() for p in sequence_predictor.parameters() if p.requires_grad)
print(f"Total trainable parameters in the whole model: {total_params}")

# Print model size
total_params = sum(p.numel() for p in sequence_predictor.parameters())
print(f"Total parameters: {total_params}")

"""## 3.2 Training loops"""

# @title Training tools
criterion_images = nn.L1Loss()
criterion_ctx = nn.MSELoss()
criterion_text = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))
optimizer = torch.optim.Adam(sequence_predictor.parameters(), lr=0.001)
# =========== ADD LEARNING RATE SCHEDULER ===========
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=2
)

# =========== ENHANCEMENT 2: TEMPORAL CONTRASTIVE LOSS ===========
class TemporalContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature

    def forward(self, context_embeddings, positive_futures, negative_futures):
        # context_embeddings: [batch, dim]
        # positive_futures: [batch, dim]
        # negative_futures: [batch, num_negatives, dim]

        # Positive similarity
        pos_sim = F.cosine_similarity(context_embeddings, positive_futures, dim=-1)
        pos_sim = pos_sim / self.temperature

        # Negative similarities
        batch_size, num_negatives, _ = negative_futures.shape
        context_expanded = context_embeddings.unsqueeze(1).expand(-1, num_negatives, -1)

        # FIX: Use .reshape() for non-contiguous tensors
        neg_sims = F.cosine_similarity(context_expanded.reshape(-1, context_embeddings.size(-1)),
                                      negative_futures.reshape(-1, negative_futures.size(-1)),
                                      dim=-1)
        # FIX: Replace .view() with .reshape() here as well
        neg_sims = neg_sims.reshape(batch_size, num_negatives) / self.temperature

        # Combine for InfoNCE
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sims], dim=1)
        labels = torch.zeros(batch_size, dtype=torch.long, device=context_embeddings.device)

        loss = F.cross_entropy(logits, labels)
        return loss

# =========== ENHANCEMENT 3: HARD NEGATIVE MINING ===========
def mine_hard_negatives(batch_idx, dataset, batch_size=8, num_negatives=3):
    """
    Simple negative mining: pick random frames from different sequences
    """
    batch_indices = list(range(batch_idx * batch_size, min((batch_idx + 1) * batch_size, len(dataset))))
    negative_samples = []

    for idx in batch_indices:
        negs = []
        for _ in range(num_negatives):
            # Pick a different sequence
            neg_idx = np.random.choice(len(dataset))
            while neg_idx == idx:
                neg_idx = np.random.choice(len(dataset))

            # Pick a random frame from that sequence
            frames = dataset[neg_idx]["images"]
            frame_idx = np.random.randint(0, len(frames))

            # Transform the image
            transform = transforms.Compose([
                transforms.Resize((60, 125)),
                transforms.ToTensor()
            ])
            neg_img = transform(frames[frame_idx])
            negs.append(neg_img)

        negative_samples.append(torch.stack(negs))

    return torch.stack(negative_samples) if negative_samples else None

# @title Training loop for the sequence predictor with tags
sequence_predictor.train()
losses = []

# =========== ADD LOSS TRACKING LISTS ===========
all_losses = []
image_losses = []
text_losses = []
tag_losses = []
epoch_numbers = []

# Get tag vocabulary sizes from dataset
tag_vocab_sizes = sp_train_dataset.get_vocab_sizes()
print(f"Tag vocabulary sizes: {tag_vocab_sizes}")

# =========== ENHANCEMENT 5: ADD CONTRASTIVE LOSS ===========
criterion_contrastive = TemporalContrastiveLoss(temperature=0.1)

# Add tag classification loss
criterion_tags = nn.CrossEntropyLoss()

for epoch in range(N_EPOCHS):
    running_loss = 0.0
    running_image_loss = 0.0
    running_text_loss = 0.0
    running_tag_loss = 0.0

    for batch_idx, (frames, descriptions, objects_seq, actions_seq, locations_seq,
                   image_target, text_target, target_objects, target_actions, target_locations) in enumerate(train_dataloader):

        # Move to device
        frames = frames.to(device)
        descriptions = descriptions.to(device)
        objects_seq = objects_seq.to(device)
        actions_seq = actions_seq.to(device)
        locations_seq = locations_seq.to(device)
        image_target = image_target.to(device)
        text_target = text_target.to(device)
        target_objects = target_objects.to(device)
        target_actions = target_actions.to(device)
        target_locations = target_locations.to(device)

        # Mine hard negatives (this function is currently unused in the contrastive loss calculation)
        negative_samples = mine_hard_negatives(
            batch_idx,
            sp_train_dataset.dataset,
            batch_size=frames.size(0),
            num_negatives=3
        )
        if negative_samples is not None:
            negative_samples = negative_samples.to(device)

        # =========== FIXED: Forward pass with tags (now returns 6 values) ===========
        pred_image_content, pred_image_context, predicted_text_logits_k, z_final, tag_features, tag_logits = sequence_predictor(
            frames, descriptions, objects_seq, actions_seq, locations_seq, text_target
        )

        # =========== IMPROVED LOSS COMPUTATION ===========
        # =========== DIFFUSION-AWARE IMAGE LOSS ===========
        if isinstance(sequence_predictor.image_decoder, DiffusionDecoder):
            # For diffusion decoder, we need different handling
            # Get text features for conditioning (use the final fused representation)
            text_features_for_diffusion = z_final

            # Forward through diffusion decoder with noise prediction
            pred_image_content, predicted_noise = sequence_predictor.image_decoder(
                z_final,
                text_features=text_features_for_diffusion,
                return_noise=True
            )

            # Diffusion loss: noise prediction loss
            target_noise = torch.randn_like(z_final)
            loss_im = F.mse_loss(predicted_noise, target_noise)

            # Add reconstruction loss for better quality
            loss_recon = F.l1_loss(pred_image_content, image_target)
            loss_im = loss_im + 0.1 * loss_recon

            # IMPORTANT: detach for metrics
            pred_image_content = pred_image_content.detach()

        else:
            # Original loss for non-diffusion decoder
            loss_mse = F.mse_loss(pred_image_content, image_target)
            loss_l1 = F.l1_loss(pred_image_content, image_target)
            loss_im = loss_mse + 0.5 * loss_l1

        running_image_loss += loss_im.item()
        # =========== END DIFFUSION LOSS ===========

        # Context loss (simplified)
        loss_context = F.mse_loss(pred_image_context.mean(), torch.tensor(0.5, device=device))

        # Text loss
        prediction_flat = predicted_text_logits_k.reshape(-1, tokenizer.vocab_size)
        target_flat = text_target[:, 1:].reshape(-1)
        loss_text = criterion_text(prediction_flat, target_flat)
        running_text_loss += loss_text.item()

        # =========== TAG CLASSIFICATION LOSS ===========
        # Split predictions for different tag types (tag_logits now comes from model)
        obj_logits = tag_logits[:, :tag_vocab_sizes['objects']] # (B, C_obj)
        act_logits = tag_logits[:, tag_vocab_sizes['objects']:tag_vocab_sizes['objects']+tag_vocab_sizes['actions']] # (B, C_act)
        loc_logits = tag_logits[:, tag_vocab_sizes['objects']+tag_vocab_sizes['actions']:] # (B, C_loc)

        # Create masks for non-padding indices
        obj_mask = (target_objects != 0) # (B, K_obj) (K_obj=5)
        act_mask = (target_actions != 0) # (B, K_act) (K_act=3)
        loc_mask = (target_locations != 0) # (B, K_loc) (K_loc=2)

        # For objects:
        obj_logits_repeated = obj_logits.unsqueeze(1).expand(-1, target_objects.size(1), -1) # (B, K_obj, C_obj)
        obj_logits_flattened = obj_logits_repeated.reshape(-1, tag_vocab_sizes['objects']) # (B * K_obj, C_obj)
        target_objects_flattened = target_objects.reshape(-1) # (B * K_obj)
        obj_mask_flattened = obj_mask.reshape(-1) # (B * K_obj)

        if obj_mask_flattened.sum() > 0: # Changed from .any() to .sum() > 0 for robustness
            loss_obj = criterion_tags(obj_logits_flattened[obj_mask_flattened], target_objects_flattened[obj_mask_flattened])
        else:
            loss_obj = torch.tensor(0.0, device=device)

        # For actions:
        act_logits_repeated = act_logits.unsqueeze(1).expand(-1, target_actions.size(1), -1) # (B, K_act, C_act)
        act_logits_flattened = act_logits_repeated.reshape(-1, tag_vocab_sizes['actions']) # (B * K_act, C_act)
        target_actions_flattened = target_actions.reshape(-1) # (B * K_act)
        act_mask_flattened = act_mask.reshape(-1) # (B * K_act)

        if act_mask_flattened.sum() > 0: # Changed from .any() to .sum() > 0 for robustness
            loss_act = criterion_tags(act_logits_flattened[act_mask_flattened], target_actions_flattened[act_mask_flattened])
        else:
            loss_act = torch.tensor(0.0, device=device)

        # For locations:
        loc_logits_repeated = loc_logits.unsqueeze(1).expand(-1, target_locations.size(1), -1) # (B, K_loc, C_loc)
        loc_logits_flattened = loc_logits_repeated.reshape(-1, tag_vocab_sizes['locations']) # (B * K_loc, C_loc)
        target_locations_flattened = target_locations.reshape(-1) # (B * K_loc)
        loc_mask_flattened = loc_mask.reshape(-1) # (B * K_loc)

        if loc_mask_flattened.sum() > 0: # Changed from .any() to .sum() > 0 for robustness
            loss_loc = criterion_tags(loc_logits_flattened[loc_mask_flattened], target_locations_flattened[loc_mask_flattened])
        else:
            loss_loc = torch.tensor(0.0, device=device)

        loss_tags = loss_obj + loss_act + loss_loc
        running_tag_loss += loss_tags.item()
        # =========== END TAG LOSS ===========

        # =========== ADD TEMPORAL CONTRASTIVE LOSS ===========
        # Get image dimensions
        batch_size, seq_len, C, H, W = frames.shape

        # Create positive pairs (context -> next frame)
        with torch.no_grad():
            # Get embeddings for next frame in sequence
            next_frames = frames[:, 1:, :, :, :]  # Take frames 1-3 as positive futures
            next_frames_flat = next_frames.reshape(-1, C, H, W)
            positive_embeddings = sequence_predictor.image_encoder(next_frames_flat)
            positive_embeddings = positive_embeddings.view(batch_size, seq_len-1, -1)

            # Sample hard negatives from other sequences in batch
            negative_embeddings = []
            # Store original training state of the encoder
            original_encoder_state = sequence_predictor.image_encoder.training
            sequence_predictor.image_encoder.eval() # Set to eval mode for single-sample inference

            for i in range(batch_size):
                # Get indices of other sequences
                other_indices = [j for j in range(batch_size) if j != i]
                # Randomly select 3 negative sequences
                num_to_pick = min(3, len(other_indices))
                negs = []
                if num_to_pick > 0: # Only pick if there are other samples available
                    neg_idxs = np.random.choice(other_indices, num_to_pick, replace=False)
                    # Take their last frame as negative
                    for idx in neg_idxs:
                        neg_frame = frames[idx, -1, :, :, :].unsqueeze(0) # neg_frame shape: [1, C, H, W]
                        neg_emb = sequence_predictor.image_encoder(neg_frame)
                        negs.append(neg_emb)
                if negs:
                    negative_embeddings.append(torch.cat(negs, dim=0))
                else:
                    # If no negatives could be picked, append dummy zero embeddings to maintain batch size
                    negative_embeddings.append(torch.zeros(3, positive_embeddings.size(-1), device=device))

            # Restore original training state
            sequence_predictor.image_encoder.train(original_encoder_state)

            negative_embeddings = torch.stack(negative_embeddings)  # [batch, num_negs, dim]

        # Compute contrastive loss
        # Check if negative_embeddings were actually generated (i.e., not all zeros from dummy case)
        if negative_embeddings.numel() > 0 and negative_embeddings.sum() != 0:
            # Use z_final as query, positive_embeddings[:, -1] as positive key
            loss_contrastive_val = criterion_contrastive(
                z_final,
                positive_embeddings[:, -1],
                negative_embeddings
            )
        else:
            loss_contrastive_val = torch.tensor(0.0, device=device)
        # =========== END CONTRASTIVE LOSS ===========

        # Add SSIM loss (maximize SSIM = minimize 1-SSIM)
        try:
            ssim_loss = 1 - StructuralSimilarityIndexMeasure(data_range=1.0).to(device)(pred_image_content, image_target)
        except Exception as e:
            print(f"SSIM calculation error: {e}")
            ssim_loss = torch.tensor(0.0, device=device)

        # Combined loss with tag weighting - USE NEW VARIABLE NAME
        loss = (loss_im +
                0.5 * loss_text +
                0.2 * loss_context +
                0.3 * loss_tags +  # Tag loss weight
                0.1 * loss_contrastive_val +  # CHANGED: loss_contrastive_val instead of loss_contrastive
                0.2 * ssim_loss)

        # Optimize
        optimizer.zero_grad()
        loss.backward()
        # Gradient clipping to prevent explosions
        torch.nn.utils.clip_grad_norm_(sequence_predictor.parameters(), max_norm=1.0)
        optimizer.step()

        running_loss += loss.item() * frames.size(0)

    # =========== ADD THIS AFTER THE INNER TRAINING LOOP ===========
    # checking model performance on validation set
    sequence_predictor.eval()
    print("Validation on training dataset")
    print("----------------")
    validation(sequence_predictor, train_dataloader)
    print("Validation on validation dataset")
    print("----------------")
    validation(sequence_predictor, val_dataloader)
    sequence_predictor.train()

    epoch_loss = running_loss / len(train_dataloader.dataset)
    losses.append(epoch_loss)

    # =========== SAVE LOSSES FOR PLOTTING ===========
    all_losses.append(epoch_loss)
    image_losses.append(running_image_loss/len(train_dataloader))
    text_losses.append(running_text_loss/len(train_dataloader))
    tag_losses.append(running_tag_loss/len(train_dataloader))
    epoch_numbers.append(epoch+1)

    scheduler.step(epoch_loss)

    print(f'Epoch [{epoch+1}/{N_EPOCHS}], Loss: {epoch_loss:.4f}')
    print(f'  Image Loss: {running_image_loss/len(train_dataloader):.4f}')
    print(f'  Text Loss: {running_text_loss/len(train_dataloader):.4f}')
    print(f'  Tag Loss: {running_tag_loss/len(train_dataloader):.4f}')

    # =========== ENHANCEMENT 10: ADD EARLY STOPPING ===========
    if 'early_stopper' in globals():
        if early_stopper(epoch_loss):
            print(f"Early stopping triggered at epoch {epoch+1}")
            break

        # =========== CURRICULUM LEARNING UPDATE ===========
    if epoch % 2 == 0 and epoch > 0:  # Update every 2 epochs after first
        sp_train_dataset.curriculum_step = min(sp_train_dataset.curriculum_step + 1,
                                              len(sp_train_dataset.sequence_lengths) - 1)
        print(f"Curriculum updated to sequence length: {sp_train_dataset.sequence_lengths[sp_train_dataset.curriculum_step]}")

    if epoch % 5 == 0:
        save_checkpoint_to_drive(sequence_predictor, optimizer, epoch, epoch_loss,
                                filename=f"sequence_predictor_with_tags.pth")

# =========== PLOT ALL LOSSES ===========
fig, ax = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Overall Loss
ax[0, 0].plot(epoch_numbers, all_losses, 'b-', linewidth=2, label='Overall Loss')
ax[0, 0].set_xlabel('Epoch')
ax[0, 0].set_ylabel('Loss')
ax[0, 0].set_title('Overall Training Loss')
ax[0, 0].grid(True, alpha=0.3)
ax[0, 0].legend()

# Plot 2: Component Losses
ax[0, 1].plot(epoch_numbers, image_losses, 'r-', linewidth=2, label='Image Loss')
ax[0, 1].plot(epoch_numbers, text_losses, 'g-', linewidth=2, label='Text Loss')
ax[0, 1].plot(epoch_numbers, tag_losses, 'b-', linewidth=2, label='Tag Loss')
ax[0, 1].set_xlabel('Epoch')
ax[0, 1].set_ylabel('Loss')
ax[0, 1].set_title('Component Losses')
ax[0, 1].grid(True, alpha=0.3)
ax[0, 1].legend()

# Plot 3: Ratio of losses
ax[1, 0].plot(epoch_numbers, [i/t if t>0 else 0 for i,t in zip(image_losses, text_losses)],
              'm-', linewidth=2, label='Image/Text Ratio')
ax[1, 0].plot(epoch_numbers, [t/tt if tt>0 else 0 for t,tt in zip(tag_losses, text_losses)],
              'c-', linewidth=2, label='Tag/Text Ratio')
ax[1, 0].set_xlabel('Epoch')
ax[1, 0].set_ylabel('Loss Ratio')
ax[1, 0].set_title('Loss Ratios Over Time')
ax[1, 0].grid(True, alpha=0.3)
ax[1, 0].legend()

# Plot 4: Combined view
ax[1, 1].plot(epoch_numbers, all_losses, 'k-', linewidth=3, label='Overall', alpha=0.7)
ax[1, 1].fill_between(epoch_numbers, 0, all_losses, alpha=0.2, color='black')
ax[1, 1].set_xlabel('Epoch')
ax[1, 1].set_ylabel('Loss')
ax[1, 1].set_title('All Losses Combined View')
ax[1, 1].grid(True, alpha=0.3)
ax[1, 1].legend()

plt.tight_layout()
plt.show()

# =========== SAVE LOSS DATA ===========
loss_data = {
    'epochs': epoch_numbers,
    'overall_loss': all_losses,
    'image_loss': image_losses,
    'text_loss': text_losses,
    'tag_loss': tag_losses
}

# Save to numpy file
np.save('training_losses.npy', loss_data)
print("Loss data saved to 'training_losses.npy'")

# Do better plots
plt.plot(losses)
plt.show()

# @title Example text reconstruction task

# Don't forget to unfreeze the model!
# Unfreeze the model by setting requires_grad=True for its parameters
for param in text_autoencoder.parameters():
    param.requires_grad = True

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(text_autoencoder.parameters(), lr=0.001)

loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))
N_EPOCHS = 5

for epoch in range(N_EPOCHS):
    text_autoencoder.train()
    epoch_loss = 0
    for description in text_dataloader:
        # Move the "sentences" to device
        input_ids = tokenizer(description, return_tensors="pt",  padding=True, truncation=True).input_ids
        input_ids = input_ids.to(device)

        # zero the grad, then forward pass
        optimizer.zero_grad()
        outputs = text_autoencoder(input_ids, input_ids)
        # compute the loss: compare 3D logits to 2D targets
        loss = loss_fn(outputs.reshape(-1, tokenizer.vocab_size), input_ids[:, 1:].reshape(-1))
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f"Epoch {epoch+1}/{N_EPOCHS}; Avg loss {epoch_loss/len(text_dataloader)}; Latest loss {loss.item()}")
    torch.save(text_autoencoder.state_dict(), f"seq2seq-epoch-{epoch+1}.pth")

# # saving checkpoint to drive
save_checkpoint_to_drive(text_autoencoder, optimizer, 3*N_EPOCHS, loss, filename = "text_autoencoder.pth")

"""# **Appendix**

This code computes the average images in case you want to use them. Notice that the average should be all zeros!
"""

# @title Computing and showing average images
N = 1000
H, W = 60, 125

# Tensors to accumulate sum (for mean) and sum of squares (for variance)
avg_images = [torch.zeros((3, H, W)) for _ in range(5)]
sum_sq_diff = [torch.zeros((3, H, W)) for _ in range(5)] # Placeholder for variance numerator
# Add Resize to the transform to ensure all images conform to H, W
transform = transforms.Compose([
  transforms.Resize((H, W)), # Resize images to HxW
  transforms.ToTensor()
])

# --- First Pass: Calculate the Sum (for Mean) ---
print("Starting Pass 1: Calculating Mean...")

for i in range(N):
    # Process sequence i
    sequence = train_dataset[i]["images"]

    for j in range(5):
        image = transform(sequence[j])
        avg_images[j] += image # Sum for mean

# Final step for mean
for j in range(5):
    avg_images[j] /= N

print("Starting Pass 2: Calculating Variance...")

for i in range(N):
    # Process sequence i
    sequence = train_dataset[i]["images"]

    for j in range(5):
        image = transform(sequence[j])

        # Calculate (Image - Mean)^2
        # Note: We detach the mean from the computation graph if it were being trained,
        # but here we're just using it as a fixed statistical value.
        diff = image - avg_images[j]
        sum_sq_diff[j] += diff * diff # Element-wise squaring

# --- Final step for Standard Deviation ---
std_images = []
for j in range(5):
    # Variance = Sum of Squared Differences / N
    variance = sum_sq_diff[j] / N

    # Standard Deviation = sqrt(Variance)
    std_dev = torch.sqrt(variance)
    std_images.append(std_dev)

print("Computation Complete. std_images is a list of 5 tensors (3x60x125).")
# You now have the 5 tensors you need for normalization (mean and std).

fig, ax = plt.subplots(1,5, figsize=(15,5))
for i in range(5):
  avg_image = avg_images[i]

  # Printing range of avg_image
  print(torch.min(avg_image), torch.max(avg_image))

  avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))
  show_image(ax[i], avg_imagen)

# Create a matrix of images with the differences between avg_images
fig, ax = plt.subplots(5,5, figsize=(15,8))

for i in range(5):
  for j in range(5):
    if i == j:
      avg_image = avg_images[i]
      avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))
      show_image(ax[i,j], avg_imagen)
    else:
      diff = avg_images[i] - avg_images[j]
      diff = (diff - torch.min(diff))/(torch.max(diff) - torch.min(diff))
      show_image(ax[i,j], diff)
    ax[i,j].set_xticks([])
    ax[i,j].set_yticks([])
plt.tight_layout()
plt.subplots_adjust(
    wspace=0, # Set horizontal space to zero
    hspace=0  # Set vertical space to zero
)